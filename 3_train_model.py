import pandas as pd
import numpy as np
import pickle
import os
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

# ==========================================
# 1. C·∫§U H√åNH & IMPORT TI·ªÜN √çCH
# ==========================================
print("--- üöÄ KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG HU·∫§N LUY·ªÜN (AI POWERED) ---")

# Import h√†m x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ file utils.py
try:
    from utils import master_pipeline
except ImportError:
    print("‚ùå L·ªñI CRITICAL: Kh√¥ng t√¨m th·∫•y file 'utils.py' ho·∫∑c h√†m 'master_pipeline'.")
    exit()

# Ki·ªÉm tra th∆∞ m·ª•c v√† file d·ªØ li·ªáu
if not os.path.exists('data/laptops_train.csv') or not os.path.exists('data/laptops_test.csv'):
    print("‚ö†Ô∏è L·ªñI: Kh√¥ng t√¨m th·∫•y file csv trong th∆∞ m·ª•c 'data/'.")
    exit()

# ==========================================
# 2. CHU·∫®N B·ªä D·ªÆ LI·ªÜU (DATA PREPARATION)
# ==========================================
print("-> üìÇ ƒêang t·∫£i d·ªØ li·ªáu...")
df_train = pd.read_csv('data/laptops_train.csv')
df_test = pd.read_csv('data/laptops_test.csv')
df = pd.concat([df_train, df_test], ignore_index=True)

print("-> üßπ ƒêang l√†m s·∫°ch d·ªØ li·ªáu (Data Cleaning via Pipeline)...")
df_clean = master_pipeline(df)

# M√£ h√≥a One-Hot (One-Hot Encoding)
# L∆∞u √Ω: Vi·ªác n√†y t·∫°o ra c√°c c·ªôt nh∆∞ 'Company_Dell', 'Company_Apple'...
df_encoded = pd.get_dummies(df_clean, columns=['Manufacturer', 'Category', 'CPU_Brand', 'GPU_Brand', 'OS'])

# T√°ch bi·∫øn ƒë·ªôc l·∫≠p (X) v√† bi·∫øn m·ª•c ti√™u (y)
X = df_encoded.drop(columns=['Price'])
y = df_encoded['Price']

# Log Transform bi·∫øn gi√° ti·ªÅn (Gi√∫p ph√¢n ph·ªëi chu·∫©n h∆°n, m√¥ h√¨nh h·ªçc t·ªët h∆°n)
y_log = np.log(y)

# Chia t·∫≠p d·ªØ li·ªáu: 85% Train - 15% Test
X_train, X_test, y_train_log, y_test_log = train_test_split(X, y_log, test_size=0.15, random_state=42)

print(f"-> K√≠ch th∆∞·ªõc d·ªØ li·ªáu hu·∫•n luy·ªán: {X_train.shape}")
print("-" * 40)

# ==========================================
# 3. HU·∫§N LUY·ªÜN & SO S√ÅNH (TRAINING & EVALUATION)
# ==========================================

# H√†m ti·ªán √≠ch ƒë·ªÉ in k·∫øt qu·∫£ ƒë√°nh gi√°
def evaluate_model(model, name, X_test, y_test_log):
    y_pred = np.exp(model.predict(X_test)) # Chuy·ªÉn ng∆∞·ª£c t·ª´ Log v·ªÅ gi√° th·ª±c t·∫ø
    actual = np.exp(y_test_log)
    r2 = r2_score(actual, y_pred)
    mae = mean_absolute_error(actual, y_pred)
    print(f"üîπ {name:<20} | R2: {r2:.4f} | MAE: {mae:,.0f} VNƒê")
    return r2

# --- MODEL 1: LINEAR REGRESSION ---
lr = LinearRegression()
lr.fit(X_train, y_train_log)
evaluate_model(lr, "Linear Regression", X_test, y_test_log)

# --- MODEL 2: RANDOM FOREST ---
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train_log)
evaluate_model(rf, "Random Forest", X_test, y_test_log)

# ==========================================
# 4. XGBOOST N√ÇNG CAO (HYPERPARAMETER TUNING)
# ==========================================
print("\n-> ‚è≥ ƒêang ch·∫°y Grid Search t·ªëi ∆∞u h√≥a XGBoost (AI Model)...")
print("   (Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t ƒë·ªÉ t√¨m tham s·ªë t·ªët nh·∫•t)")

xgb_base = xgb.XGBRegressor(random_state=42)

# L∆∞·ªõi tham s·ªë "h·∫°ng n·∫∑ng" ƒë·ªÉ ƒë·∫°t ƒë·ªô ch√≠nh x√°c cao
param_grid = {
    'n_estimators': [1000, 1500],       # S·ªë l∆∞·ª£ng c√¢y l·ªõn ƒë·ªÉ h·ªçc s√¢u
    'learning_rate': [0.05],            # T·ªëc ƒë·ªô h·ªçc ch·∫≠m v√† ch·∫Øc
    'max_depth': [6, 7],                # ƒê·ªô s√¢u v·ª´a ƒë·ªß ƒë·ªÉ b·∫Øt pattern ph·ª©c t·∫°p
    'subsample': [0.8],                 # Ch·ªâ h·ªçc 80% d·ªØ li·ªáu m·ªói c√¢y ƒë·ªÉ tr√°nh Overfitting
    'colsample_bytree': [0.8]           # (M·ªõi) Ch·ªâ d√πng 80% s·ªë c·ªôt ƒë·∫∑c tr∆∞ng m·ªói c√¢y
}

grid_search = GridSearchCV(
    estimator=xgb_base,
    param_grid=param_grid,
    cv=3, verbose=1, n_jobs=-1, scoring='r2'
)

grid_search.fit(X_train, y_train_log)

best_xgb = grid_search.best_estimator_
print(f"‚úÖ Tham s·ªë t·ªëi ∆∞u: {grid_search.best_params_}")

# ƒê√°nh gi√° Model t·ªët nh·∫•t
r2_xgb = evaluate_model(best_xgb, "XGBoost (Tuned)", X_test, y_test_log)

# ==========================================
# 5. KI·ªÇM TRA ƒê·ªò L·ªÜCH (OVERFITTING CHECK)
# ==========================================
print("\n-> üîç Ki·ªÉm tra ƒë·ªô ·ªïn ƒë·ªãnh m√¥ h√¨nh (Overfitting Check):")
y_pred_train = np.exp(best_xgb.predict(X_train))
r2_train = r2_score(np.exp(y_train_log), y_pred_train)

print(f"   + ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p TRAIN: {r2_train:.2%} (L√Ω thuy·∫øt)")
print(f"   + ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p TEST:  {r2_xgb:.2%}  (Th·ª±c t·∫ø)")

if r2_train - r2_xgb > 0.15:
    print("   ‚ö†Ô∏è C·∫¢NH B√ÅO: M√¥ h√¨nh c√≥ d·∫•u hi·ªáu h·ªçc v·∫πt (Overfitting).")
else:
    print("   ‚úÖ ƒê√ÅNH GI√Å: M√¥ h√¨nh h·ªçc t·ªët, ƒë·ªô ·ªïn ƒë·ªãnh cao.")

print("-" * 40)

# ==========================================
# 6. L∆ØU MODEL (SAVING)
# ==========================================
if not os.path.exists('models'):
    os.makedirs('models')

print("üíæ ƒêang l∆∞u m√¥ h√¨nh v√†o th∆∞ m·ª•c 'models/'...")

# 1. L∆∞u Model AI (XGBoost)
with open('models/laptop_price_model.pkl', 'wb') as f:
    pickle.dump(best_xgb, f)

# 2. L∆∞u danh s√°ch c·ªôt (R·∫•t quan tr·ªçng cho Web App)
with open('models/model_columns.pkl', 'wb') as f:
    pickle.dump(X.columns.tolist(), f)

print(f"‚úÖ HO√ÄN T·∫§T! ƒê√£ l∆∞u model v·ªõi ƒë·ªô ch√≠nh x√°c R2 = {r2_xgb:.2%}")
print("   S·∫µn s√†ng t√≠ch h·ª£p v√†o Streamlit App.")