import pandas as pd
import numpy as np
import pickle
import os
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
import optuna
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score

# ==========================================
# 1. C·∫§U H√åNH & IMPORT TI·ªÜN √çCH
# ==========================================
print("--- üöÄ KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG HU·∫§N LUY·ªÜN (AI POWERED) ---")

# Import h√†m x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ file utils.py
try:
    from utils import master_pipeline
except ImportError:
    print("‚ùå L·ªñI CRITICAL: Kh√¥ng t√¨m th·∫•y file 'utils.py' ho·∫∑c h√†m 'master_pipeline'.")
    exit()

# Ki·ªÉm tra th∆∞ m·ª•c v√† file d·ªØ li·ªáu
if not os.path.exists('data/laptops_train.csv') or not os.path.exists('data/laptops_test.csv'):
    print("‚ö†Ô∏è L·ªñI: Kh√¥ng t√¨m th·∫•y file csv trong th∆∞ m·ª•c 'data/'.")
    exit()

# ==========================================
# 2. CHU·∫®N B·ªä D·ªÆ LI·ªÜU (DATA PREPARATION)
# ==========================================
print("-> üìÇ ƒêang t·∫£i d·ªØ li·ªáu...")
df_train = pd.read_csv('data/laptops_train.csv')
df_test = pd.read_csv('data/laptops_test.csv')
df = pd.concat([df_train, df_test], ignore_index=True)

print("-> üßπ ƒêang l√†m s·∫°ch d·ªØ li·ªáu (Data Cleaning via Pipeline)...")
df_clean = master_pipeline(df)

# M√£ h√≥a One-Hot (One-Hot Encoding)
# L∆∞u √Ω: Vi·ªác n√†y t·∫°o ra c√°c c·ªôt nh∆∞ 'Company_Dell', 'Company_Apple'...
df_encoded = pd.get_dummies(df_clean, columns=['Manufacturer', 'Category', 'CPU_Brand', 'GPU_Brand', 'OS'])

# T√°ch bi·∫øn ƒë·ªôc l·∫≠p (X) v√† bi·∫øn m·ª•c ti√™u (y)
X = df_encoded.drop(columns=['Price'])
y = df_encoded['Price']

# Log Transform bi·∫øn gi√° ti·ªÅn (Gi√∫p ph√¢n ph·ªëi chu·∫©n h∆°n, m√¥ h√¨nh h·ªçc t·ªët h∆°n)
y_log = np.log(y)

# Chia t·∫≠p d·ªØ li·ªáu: 85% Train - 15% Test
X_train, X_test, y_train_log, y_test_log = train_test_split(X, y_log, test_size=0.15, random_state=42)

print(f"-> K√≠ch th∆∞·ªõc d·ªØ li·ªáu hu·∫•n luy·ªán: {X_train.shape}")
print("-" * 40)

# ==========================================
# 3. HU·∫§N LUY·ªÜN & SO S√ÅNH (TRAINING & EVALUATION)
# ==========================================

# H√†m ti·ªán √≠ch ƒë·ªÉ in k·∫øt qu·∫£ ƒë√°nh gi√°
def evaluate_model(model, name, X_test, y_test_log):
    y_pred = np.exp(model.predict(X_test)) # Chuy·ªÉn ng∆∞·ª£c t·ª´ Log v·ªÅ gi√° th·ª±c t·∫ø
    actual = np.exp(y_test_log)
    r2 = r2_score(actual, y_pred)
    mae = mean_absolute_error(actual, y_pred)
    print(f"üîπ {name:<20} | R2: {r2:.4f} | MAE: {mae:,.0f} VNƒê")
    return r2

# --- MODEL 1: LINEAR REGRESSION ---
lr = LinearRegression()
lr.fit(X_train, y_train_log)
evaluate_model(lr, "Linear Regression", X_test, y_test_log)

# --- MODEL 2: RANDOM FOREST ---
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train_log)
evaluate_model(rf, "Random Forest", X_test, y_test_log)

# ==========================================
# 4. XGBOOST N√ÇNG CAO V·ªöI OPTUNA + EARLY STOPPING
# ==========================================
print("\n-> ‚è≥ ƒêang kh·ªüi ƒë·ªông Optuna ƒë·ªÉ t√¨m tham s·ªë t·ªëi ∆∞u (AI Mode)...")


# --- A. ƒê·ªäNH NGHƒ®A H√ÄM M·ª§C TI√äU (OBJECTIVE FUNCTION) ---
def objective(trial):
    params = {
        'n_estimators': 1000,
        # Cho ph√©p h·ªçc nhanh h∆°n m·ªôt ch√∫t
        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2),

        # Cho ph√©p c√¢y s√¢u h∆°n m·ªôt ch√∫t ƒë·ªÉ b·∫Øt ƒë∆∞·ª£c c√°c m·∫´u kh√≥
        'max_depth': trial.suggest_int('max_depth', 5, 10),

        # Gi·ªØ nguy√™n ƒë·ªÉ ch·ªëng h·ªçc v·∫πt
        'subsample': trial.suggest_float('subsample', 0.7, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),

        # GI·∫¢M H√åNH PH·∫†T: Cho ph√©p model linh ho·∫°t h∆°n
        # Tr∆∞·ªõc ƒë√¢y cho t·ªõi 10.0, gi·ªù ch·ªâ cho t·ªëi ƒëa 2.0 ho·∫∑c 3.0
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 3.0),

        'n_jobs': -1,
        'random_state': 42,
        'verbosity': 0
    }

    cv_scores = []
    kf = KFold(n_splits=5, shuffle=True, random_state=42)

    for train_idx, val_idx in kf.split(X_train):
        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_train_fold, y_val_fold = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]

        model = xgb.XGBRegressor(**params, early_stopping_rounds=100)  # TƒÉng ki√™n nh·∫´n l√™n 100

        model.fit(
            X_train_fold, y_train_fold,
            eval_set=[(X_val_fold, y_val_fold)],
            verbose=False
        )

        preds = model.predict(X_val_fold)
        score = r2_score(y_val_fold, preds)
        cv_scores.append(score)

    return np.mean(cv_scores)

# --- B. CH·∫†Y T·ªêI ∆ØU H√ìA ---
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

print(f"\n‚úÖ ƒê√£ t√¨m th·∫•y tham s·ªë t·ªët nh·∫•t:")
print(f"   -> R2 trung b√¨nh (Cross-Validation): {study.best_value:.4f}")
print(f"   -> B·ªô tham s·ªë: {study.best_params}")

# ==========================================
# 5. HU·∫§N LUY·ªÜN L·∫†I MODEL CU·ªêI C√ôNG (FINAL TRAINING)
# ==========================================
print("\n-> üöÄ ƒêang hu·∫•n luy·ªán l·∫°i model t·ªët nh·∫•t tr√™n to√†n b·ªô t·∫≠p Train...")

best_params = study.best_params
best_params['n_estimators'] = 1000
best_params['n_jobs'] = -1
best_params['random_state'] = 42

# === S·ª¨A L·ªñI T·∫†I ƒê√ÇY (B∆∞·ªõc 2) ===
# ƒê∆∞a early_stopping_rounds v√†o constructor c·ªßa model cu·ªëi c√πng
final_model = xgb.XGBRegressor(**best_params, early_stopping_rounds=100)

# X√≥a early_stopping_rounds kh·ªèi h√†m fit
final_model.fit(
    X_train, y_train_log,
    eval_set=[(X_test, y_test_log)],
    verbose=False
)

evaluate_model(final_model, "XGBoost (Optuna)", X_test, y_test_log)

# ==========================================
# 6. KI·ªÇM TRA OVERFITTING & L∆ØU MODEL
# ==========================================
print("\n-> üîç Ki·ªÉm tra ƒë·ªô ·ªïn ƒë·ªãnh m√¥ h√¨nh (Overfitting Check):")
y_pred_train = np.exp(final_model.predict(X_train))
r2_train = r2_score(np.exp(y_train_log), y_pred_train)

y_pred_test_final = np.exp(final_model.predict(X_test))
r2_test_final = r2_score(np.exp(y_test_log), y_pred_test_final)

print(f"   + ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p TRAIN: {r2_train:.2%}")
print(f"   + ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p TEST:  {r2_test_final:.2%}")

diff = r2_train - r2_test_final
if diff > 0.15:
    print(f"   ‚ö†Ô∏è C·∫¢NH B√ÅO: Ch√™nh l·ªách {diff:.2%} -> V·∫´n c√≤n d·∫•u hi·ªáu Overfitting nh·∫π.")
else:
    print(f"   ‚úÖ TUY·ªÜT V·ªúI: Ch√™nh l·ªách {diff:.2%} -> Model h·ªçc r·∫•t ·ªïn ƒë·ªãnh!")

if not os.path.exists('models'):
    os.makedirs('models')

print("\nüíæ ƒêang l∆∞u m√¥ h√¨nh v√†o th∆∞ m·ª•c 'models/'...")
with open('models/laptop_price_model.pkl', 'wb') as f:
    pickle.dump(final_model, f)

with open('models/model_columns.pkl', 'wb') as f:
    pickle.dump(X.columns.tolist(), f)

print("‚úÖ HO√ÄN T·∫§T TO√ÄN B·ªò QU√Å TR√åNH!")