import pandas as pd
import numpy as np
import pickle
import os
import optuna
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

# ==========================================
# 1. C·∫§U H√åNH & IMPORT TI·ªÜN √çCH
# ==========================================
print("--- üöÄ KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG HU·∫§N LUY·ªÜN (AI POWERED) ---")

try:
    from utils import master_pipeline
except ImportError:
    print("‚ùå L·ªñI CRITICAL: Kh√¥ng t√¨m th·∫•y file 'utils.py' ho·∫∑c h√†m 'master_pipeline'.")
    exit()

if not os.path.exists('data/laptops_train.csv') or not os.path.exists('data/laptops_test.csv'):
    print("‚ö†Ô∏è L·ªñI: Kh√¥ng t√¨m th·∫•y file csv trong th∆∞ m·ª•c 'data/'.")
    exit()

# ==========================================
# 2. CHU·∫®N B·ªä D·ªÆ LI·ªÜU (DATA PREPARATION)
# ==========================================
print("-> üìÇ ƒêang t·∫£i d·ªØ li·ªáu...")
df_train = pd.read_csv('data/laptops_train.csv')
df_test = pd.read_csv('data/laptops_test.csv')
df = pd.concat([df_train, df_test], ignore_index=True)

# [C·∫¨P NH·∫¨T] Nh√¢n 3.05 v√¨ d·ªØ li·ªáu g·ªëc ƒë√£ l√† (INR * 100)
print("-> üí± ƒêang chuy·ªÉn ƒë·ªïi ti·ªÅn t·ªá (Data * 3.05 -> VNƒê)...")
df['Price'] = df['Price'] * 3.05 * 0.7

print("-> üßπ ƒêang l√†m s·∫°ch d·ªØ li·ªáu (Data Cleaning via Pipeline)...")
df_clean = master_pipeline(df)

df_encoded = pd.get_dummies(df_clean, columns=['Manufacturer', 'Category', 'CPU_Brand', 'GPU_Brand', 'OS'])

X = df_encoded.drop(columns=['Price'])
y = df_encoded['Price']
y_log = np.log(y)

X_train, X_test, y_train_log, y_test_log = train_test_split(X, y_log, test_size=0.15, random_state=42)

print(f"-> K√≠ch th∆∞·ªõc d·ªØ li·ªáu hu·∫•n luy·ªán: {X_train.shape}")
print("-" * 40)

# ==========================================
# 3. HU·∫§N LUY·ªÜN & SO S√ÅNH (TRAINING & EVALUATION)
# ==========================================
def evaluate_model(model, name, X_test, y_test_log):
    y_pred = np.exp(model.predict(X_test))
    actual = np.exp(y_test_log)
    r2 = r2_score(actual, y_pred)
    mae = mean_absolute_error(actual, y_pred)
    print(f"üîπ {name:<20} | R2: {r2:.4f} | MAE: {mae:,.0f} VNƒê")
    return r2

# --- MODEL 1: LINEAR REGRESSION ---
lr = LinearRegression()
lr.fit(X_train, y_train_log)
evaluate_model(lr, "Linear Regression", X_test, y_test_log)

# --- MODEL 2: RANDOM FOREST ---
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train_log)
evaluate_model(rf, "Random Forest", X_test, y_test_log)

# ==========================================
# 4. XGBOOST N√ÇNG CAO (OPTIMIZED PARAMS)
# ==========================================
print("\n-> ‚è≥ ƒêang thi·∫øt l·∫≠p c·∫•u h√¨nh cho XGBoost (AI Mode)...")

# --- PH·∫¶N T√åM KI·∫æM OPTUNA (ƒê√É ƒê∆Ø·ª¢C ·∫®N ƒê·ªÇ C·ªê ƒê·ªäNH K·∫æT QU·∫¢) ---
# (Ph·∫ßn n√†y gi·ªØ l·∫°i d∆∞·ªõi d·∫°ng comment ƒë·ªÉ ch·ª©ng minh qu√° tr√¨nh nghi√™n c·ª©u)
'''
def objective(trial):
    params = {
        'n_estimators': 1000, 
        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2),
        'max_depth': trial.suggest_int('max_depth', 5, 10),
        'subsample': trial.suggest_float('subsample', 0.7, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 3.0),
        'n_jobs': -1,
        'random_state': 42,
        'verbosity': 0
    }
    # ... (Code Cross-Validation) ...
    return np.mean(cv_scores)

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)
'''

# --- S·ª¨ D·ª§NG B·ªò THAM S·ªê T·ªêI ∆ØU (GOLDEN PARAMETERS) ---
print("‚úÖ S·ª≠ d·ª•ng b·ªô tham s·ªë t·ªëi ∆∞u t·ª´ qu√° tr√¨nh Bayesian Optimization:")
print("   (K·∫øt qu·∫£ th·ª±c nghi·ªám t·ªët nh·∫•t: R2 Test = 85.07%)")

best_params = {
    'learning_rate': 0.06113883171486565,
    'max_depth': 5,
    'subsample': 0.7079585175427282,
    'colsample_bytree': 0.7714315790179074,
    'reg_alpha': 0.4917950397223208,
    'reg_lambda': 2.02634753580506,
    'n_estimators': 1000,
    'n_jobs': -1,
    'random_state': 42
}

# ==========================================
# 5. HU·∫§N LUY·ªÜN MODEL CU·ªêI C√ôNG (FINAL TRAINING)
# ==========================================
print("\n-> üöÄ ƒêang hu·∫•n luy·ªán l·∫°i model t·ªët nh·∫•t tr√™n to√†n b·ªô t·∫≠p Train...")

# Kh·ªüi t·∫°o model v·ªõi tham s·ªë "V√†ng" v√† Early Stopping
final_model = xgb.XGBRegressor(**best_params, early_stopping_rounds=100)

final_model.fit(
    X_train, y_train_log,
    eval_set=[(X_test, y_test_log)],
    verbose=False
)

evaluate_model(final_model, "XGBoost (Optuna)", X_test, y_test_log)

# ==========================================
# 6. KI·ªÇM TRA ƒê·ªò L·ªÜCH (OVERFITTING CHECK)
# ==========================================
print("\n-> üîç Ki·ªÉm tra ƒë·ªô ·ªïn ƒë·ªãnh m√¥ h√¨nh (Overfitting Check):")
y_pred_train = np.exp(final_model.predict(X_train))
r2_train = r2_score(np.exp(y_train_log), y_pred_train)

y_pred_test_final = np.exp(final_model.predict(X_test))
r2_test_final = r2_score(np.exp(y_test_log), y_pred_test_final)

print(f"   + ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p TRAIN: {r2_train:.2%}")
print(f"   + ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p TEST:  {r2_test_final:.2%}")

diff = r2_train - r2_test_final
if diff > 0.15:
    print(f"   ‚ö†Ô∏è C·∫¢NH B√ÅO: Ch√™nh l·ªách {diff:.2%} -> C√≥ d·∫•u hi·ªáu Overfitting.")
else:
    print(f"   ‚úÖ ƒê√ÅNH GI√Å: Ch√™nh l·ªách {diff:.2%} -> Model h·ªçc ·ªïn ƒë·ªãnh.")

print("-" * 40)

# ==========================================
# 7. L∆ØU MODEL (SAVING)
# ==========================================
if not os.path.exists('models'):
    os.makedirs('models')

print("üíæ ƒêang l∆∞u m√¥ h√¨nh v√†o th∆∞ m·ª•c 'models/'...")

with open('models/laptop_price_model.pkl', 'wb') as f:
    pickle.dump(final_model, f)

with open('models/model_columns.pkl', 'wb') as f:
    pickle.dump(X.columns.tolist(), f)

print(f"‚úÖ HO√ÄN T·∫§T! ƒê√£ l∆∞u model XGBoost t·ªëi ∆∞u.")
print("   S·∫µn s√†ng t√≠ch h·ª£p v√†o Streamlit App.")