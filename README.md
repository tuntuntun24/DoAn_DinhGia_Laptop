# ğŸ’» Äá»“ Ãn NghiÃªn Cá»©u: Dá»± ÄoÃ¡n GiÃ¡ Laptop (XGBoost & Optuna)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Machine Learning](https://img.shields.io/badge/Model-XGBoost_Optimized-orange)
![App](https://img.shields.io/badge/Web_App-Streamlit-red)

## ğŸ“– Giá»›i thiá»‡u
Há»‡ thá»‘ng Ä‘á»‹nh giÃ¡ Laptop tá»± Ä‘á»™ng sá»­ dá»¥ng Machine Learning. Äá»“ Ã¡n táº­p trung giáº£i quyáº¿t bÃ i toÃ¡n dá»¯ liá»‡u nhá» (~1300 máº«u) báº±ng thuáº­t toÃ¡n **XGBoost** káº¿t há»£p tá»‘i Æ°u hÃ³a **Bayesian (Optuna)** Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao vÃ  chá»‘ng Overfitting.

## ğŸ“Š Dá»¯ liá»‡u & PhÆ°Æ¡ng phÃ¡p
* **Dá»¯ liá»‡u:** 1300 dÃ²ng, cÃ¡c Ä‘áº·c trÆ°ng: CPU, RAM, GPU, MÃ n hÃ¬nh, Trá»ng lÆ°á»£ng...
* **Xá»­ lÃ½:** LÃ m sáº¡ch, chuáº©n hÃ³a Ä‘Æ¡n vá»‹, One-Hot Encoding, Log Transformation cho biáº¿n giÃ¡ (Price).

## ğŸ”¬ Thá»±c nghiá»‡m & Káº¿t quáº£
NhÃ³m Ä‘Ã£ thá»­ nghiá»‡m 3 mÃ´ hÃ¬nh vÃ  Ã¡p dá»¥ng ká»¹ thuáº­t **Fine-tuning** (tinh chá»‰nh) tham sá»‘ chuyÃªn sÃ¢u:

| MÃ´ hÃ¬nh | R2 Score (Test) | MAE (Sai sá»‘) | ÄÃ¡nh giÃ¡ |
| :--- | :--- | :--- | :--- |
| Linear Regression | 70.35% | ~2.11 tr VNÄ | Underfitting |
| Random Forest | 82.66% | ~1.63 tr VNÄ | Tá»‘t |
| **XGBoost (Final)** | **85.07%** | **~1.51 tr VNÄ** | **Tá»‘t nháº¥t** |

> **Äiá»ƒm nháº¥n:** Sá»­ dá»¥ng **Optuna** Ä‘á»ƒ tÃ¬m bá»™ tham sá»‘ tá»‘i Æ°u vÃ  **Early Stopping** Ä‘á»ƒ kiá»ƒm soÃ¡t Overfitting (ChÃªnh lá»‡ch Train/Test chá»‰ ~11%).

## ğŸš€ CÃ i Ä‘áº·t & Sá»­ dá»¥ng
1. **CÃ i Ä‘áº·t thÆ° viá»‡n:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Huáº¥n luyá»‡n láº¡i (Optional):**
   ```bash
   python 3_train_model.py
   ```
   *(File nÃ y Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t bá»™ tham sá»‘ tá»‘i Æ°u nháº¥t, khÃ´ng cáº§n cháº¡y láº¡i Optuna)*

3. **Cháº¡y Web App:**
   ```bash
   streamlit run app.py
   ```

---
## ğŸ‘¨â€ğŸ’» TÃ¡c giáº£
* **Sinh viÃªn:** [Äiá»n TÃªn Báº¡n]
* **TrÆ°á»ng:** Äáº¡i há»c CÃ´ng nghiá»‡p HÃ  Ná»™i (HaUI)
* **Äá»“ Ã¡n:** NghiÃªn cá»©u Khoa há»c / Tá»‘t nghiá»‡p